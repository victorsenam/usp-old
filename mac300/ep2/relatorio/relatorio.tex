%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Arsclassica Article
% LaTeX Template
% Version 1.1 (10/6/14)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% Lorenzo Pantieri (http://www.lorenzopantieri.net) with extensive modifications by:
% Vel (vel@latextemplates.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[
10pt, % Main document font size
a4paper, % Paper type, use 'letterpaper' for US Letter paper
oneside, % One page layout (no page indentation)
%twoside, % Two page layout (page indentation for binding and different headers)
headinclude,footinclude, % Extra spacing for the header and footer
BCOR5mm, % Binding correction
]{scrartcl}

\input{structure.tex} % Include the structure.tex file which specified the document structure and layout

\hyphenation{Fortran hy-phen-ation} % Specify custom hyphenation points in words with dashes where you would like hyphenation to occur, or alternatively, don't put any dashes in a word to stop hyphenation altogether

%----------------------------------------------------------------------------------------
%	TITLE AND AUTHOR(S)
%----------------------------------------------------------------------------------------

\title{\normalfont\spacedallcaps{Método dos Gradientes Conjugados}} % The article title
\subtitle{\normalfont\spacedallcaps{EP2 - MAC0300}}

\author{\spacedlowsmallcaps{Victor Sena Molero (8941317)}} % The article author(s) - author affiliations need to be specified in the AUTHOR AFFILIATIONS block

%----------------------------------------------------------------------------------------

\begin{document}

%----------------------------------------------------------------------------------------
%	HEADERS
%----------------------------------------------------------------------------------------

\renewcommand{\sectionmark}[1]{\markright{\spacedlowsmallcaps{#1}}} % The header for all pages (oneside) or for even pages (twoside)
%\renewcommand{\subsectionmark}[1]{\markright{\thesubsection~#1}} % Uncomment when using the twoside option - this modifies the header on odd pages
\lehead{\mbox{\llap{\small\thepage\kern1em\color{halfgray} \vline}\color{halfgray}\hspace{0.5em}\rightmark\hfil}} % The header style

\pagestyle{scrheadings} % Enable the headers specified in this block

%----------------------------------------------------------------------------------------
%	TABLE OF CONTENTS & LISTS OF FIGURES AND TABLES
%----------------------------------------------------------------------------------------

\maketitle % Print the title/author/date block

\setcounter{tocdepth}{2} % Set the depth of the table of contents to show sections and subsections only

\tableofcontents % Print the table of contents

%\listoffigures % Print the list of figures
%\listoftables % Print the list of tables

%----------------------------------------------------------------------------------------
%	ABSTRACT
%----------------------------------------------------------------------------------------

\section*{Abstract} % This section will not appear in the table of contents due to the star (\section*)

Este é um relatório sobre a implementação do método dos Gradientes Conjugados para encontrar um $x \in \mathbb{R}^n$ tal que $Ax = b$ onde $A \in \mathbb{R}^{n \times n}$ é esparsa, positiva definida e conhecida e $b \in \mathbb{R}^n$ é conhecida.

%----------------------------------------------------------------------------------------

\newpage % Start the article content on the second page, remove this if you have a longer abstract that goes onto the second page

%----------------------------------------------------------------------------------------
%	INTRODUCTION
%----------------------------------------------------------------------------------------

\section{Introdução}
Durante o semestre foram estudadas várias formas de se resolver sistemas da forma $Ax = b$, é fácil perceber que, com hipóteses mais fortes, esse tipo de cálculo se torna mais fácil. Por exemplo, se $A$ for positiva definida, podemos aplicar a decomposição de Cholesky e evitar o cálculo da decomposição LU, que funciona, também, no caso onde $A$ é positiva definida. \\
O método dos Gradientes Conjugados tem, também, como hipótese, uma $A$ positiva definida. Porém, esse método permite que propriedades da matriz $A$, como, por exemplo, a esparsidade, sejam aproveitados. \\
O objetivo desse relatório é, então, explicitar o que foi entendido sobre o método em questão, mostrar uma forma de aproveitar a esparsidade da matriz no método dos Gradientes Conjugados e discutir a complexidade desse método, comparando o com a decomposição de Cholesky, já que esta é derivada das mesmas hipóteses que o método dos Gradientes conjugados. Além disso, pretendo explicar o que foi feito em outras partes do desenvolvimento, como, por exemplo, o método para gerar matrizes positivas definidas esparsas. \\
Para tentar entender o método, foram estudados dois textos, o livro \textit{Numerical Linear Algebra}\cite{trefethen1997numerical}, como indicado no enunciado do EP e um paper intitulado \textit{Painless Conjugate Gradient}\cite{shewchuk1994introduction}.
 
%----------------------------------------------------------------------------------------
%   GRADIENTES CONJUGADOS
%----------------------------------------------------------------------------------------

\section{Gradientes Conjugados}
O método dos Gradientes Conjugados, em \cite{shewchuk1994introduction} é apresentado como uma derivação dos métodos \textit{Steepest Descent} e \textit{Conjugate Directions}. Vou introduzir ambos brevemente aqui, para mostrar como eu entendi o método dos Gradientes Conjugados.

\subsection{Steepest Descent}
O método do \textit{Steepest Descent} busca encontrar o $x$ que tem a forma quadrática mínima com a matriz $A$ e o vetor $b$ dados no problema. \\
A forma quadrática de um vetor é uma função escalar e quadrática de um vetor da forma ~\cite[p.~2]{shewchuk1994introduction}
$$ f(x) = \frac{1}{2}x^{T}Ax - b^{T}x + c $$ 
E o gradiente da forma quadrática é simplesmente ~\cite[p.~3]{shewchuk1994introduction}
$$ f'(x) = Ax - b $$ 
Assim, é fácil ver que se $ f'(x) = 0 $ então
$$ Ax = b $$
resolvendo o problema. \\
Para uma $A$ positiva definida, é possível mostrar que só existe um mínimo para a forma quadrática ~\cite[p.~5]{shewchuk1994introduction} e não existe máximo, assim, calcular $f'(x) = 0$ é simplesmente minimizar a função $f(x)$ no seu único ponto de mínimo. Assim, seguir o gradiente maximiza a função, se seguirmos a direção oposta ao gradiente, estaremos nos aproximando cada vez mais do mínimo da função. E é nisso que o método se baseia. \\
Assim, o método chuta um ponto $x_0$ e, para calcular $x_{i+1}$, para todo $i$, vai na direção oposta à função $f'(x_i)$ o que garante que, com o tamanho certo do passo nessa direção, $\lim_{i \to \infty} f'(x_i) = 0$. \\
O problema é que este método pode dar passos na mesma direção que métodos anteriores, pois a direção dos passos é definida pelo gradiente de $f$. De fato, isso acontece na prática. O método de \textit{Conjugate Directions} impede que isso aconteça.

\subsection{Conjugate Directions}
Como dito acima, este método garante que nenhum as direções de cada passo sejam diferentes entre si. Para fazer isso, é introduzida a ideia de direção de busca. O método usa direções de busca $d_i$ diferentes entre si e sempre minimiza $f'(x_{i+1})$ com alterações do tipo $x_{i+1} = x_i + \alpha_i d_i$ para cada passo $i$. \\
Para isso, as direções geradas são \textit{A-ortogonais} entre si, ou seja, são tais que 
$$ \forall i,j \text{ t.q. } i \neq j\text{, } d_i^{T}Ad_j = 0 $$
Isso faz com que nenhum passo seja dado na mesma direção, já que elas são todas diferentes, e também faz com que as direções $d_0, d_1, \dots, d_i$ gerem um espaço vetorial de dimensão $i$, que chamaremos $\mathcal{D}_i$, como em \cite{shewchuk1994introduction}. Logo, se $x$ tem dimensão $n$, $x \in \mathcal{D}_n$. E já que $f'(x)$ é minimizado a cada iteração, $x_i$ é tal que $\forall p \in \mathcal{D}_i f(x_i) \leq f(p)$, ou seja, $x_i$ é o ponto mínimo de $f$ para o subespaço $\mathcal{D}_i$. \\
Agora falta conseguirmos $n$ direções de busca ${d_i}$ que são \textit{A-ortogonais} (ou seja, conjugadas) entre si. Para isso, o método de Conjugate Directions usa o processo de \textit{Gram-Schmidt} \cite[p.~25]{shewchuk1994introduction}. O problema é que, para aplicar esse método, todos os vetores já gerados devem ser guardados e percorridos completamente para gerar o próximo vetor, assim, naturalmente, o processo custa, em quantidade de operações $O(n^3)$ e $O(n^2)$ em espaço. \\

\subsection{Espaços de Krylov e o Método dos Gradientes Conjugados}
Felizmente, existe uma maneira mais eficiente de gerar as direções de busca ${d_i}$. É isso que o método dos gradientes conjugados apresenta com os espaços de Krylov. Denotaremos por $\mathcal{K}_i$ um subespaço de Krylov de dimensão $i$. \\
A facilidade para se calcular $\mathcal{K}_{i+1}$ dado $\mathcal{K}_{i}$ fez com que estes subespaços sejam a base pra vários algoritmos iterativos para resolução de sistemas lineares e para o cálculo de autovalores, como mostrado em \cite{trefethen1997numerical}. Dados uma matriz $A$ e um vetor $b$, o espaço de $\mathcal{K}_i$ tem como base $[b, Ab, A^2b, \dots, A^{i-1}b]$ ~\cite[p.~245]{trefethen1997numerical}. \\
Assim, o método dos Gradientes Conjugados é exatamente o \textit{Conjugate Directions} que usa o subespaço de Krylov para gerar direções de busca ortogonais entre si. \\

\subsection{O Algoritmo}
Agora temos tudo o que precisamos para derivar o algoritmo. Primeiro, vamos definir quais são as informações que precisamos ter em cada passo e o que elas significam. \\
\begin{description}
    \item[$A$] \hfill \\
    Entrada do Problema. $A \in \mathbb{R}^{n \times n}$ é simétrica, positiva definida e tem $m$ elementos não-nulos (esparsa).
    \item[$b$] \hfill \\
    Entrada do Problema. $A \in \mathbb{R}^{n}$.
    \item[$x_i$] \hfill \\
    Aproximação atual do valor de $x$ tal que $Ax = b$.
    \item[$r_i$] \hfill \\
    Residual de $b$, é usado para gerar a direção de Busca.
    \item[$d_i$] \hfill \\
    Direção de Busca. Onde serão gerados, indiretamente dos espaços $\mathcal{K}_i$.
    \item[$\beta_i$] \hfill \\
    Razão entre a norma do residual no passo $i$ e $i-1$, mostra o quão mais próximo da resposta $x$ $x_i$ está em relação a $x_{i-1}$.
\end{description}
O algoritmo, então, é o seguinte: ~\cite[p.~294]{trefethen1997numerical}
\begin{algorithm}[]
    $x_0 = 0, r_0 = b, d_0 = r_0$ \\
    \For{$i = 1,2,\dots,n$}{
        $\alpha_i = (r_{i-1}^Tr_{i-1})/(d_{i-1}^T A d_{i-1})$ \\
        $x_i = x_{i-1} + \alpha_i d_{i-1} $ \\
        $r_i = r_{i-1} - \alpha_i A d_{i-1} $ \\
        $\beta_i = (r_{i-1}^Tr_{i-1})/(r_{i}^Tr_{i})$ \\
        $d_i = r_i + \beta_i d_{i-1}$
    }
\end{algorithm}

\subsection{Observações}
Algumas observações interesantes podem ser feitas sobre o algoritmo. \\
Os espaços $\mathcal{K}_i$ não são explícitamente calculados. Mas eles são deles que partem as direções de busca. É fácil ver que, de fato, a direção de busca é uma combinação linear de $b, Ab, A(Ab), \dots, A^{i-1}$ em todo passo $i$. Inicialmente $d_0$ = $b$, ou seja, $d_0 \in \mathcal{K}_0$, como esperado. No passo $i$, calculamos $r_i$ e, com ele, calculamos $d_i$. É fácil ver que, no primeiro passo:
$$ d_1 = b - \alpha_1 Ab + \beta_1 b = (1+\beta_1)b - \alpha_1 Ab $$
Logo, $d_1$ é uma combinação linear de $Ab$ e $b$, pertencendo a $\mathcal{K}_1$. Isso continua por toda a execução do algoritmo. \\
Poderíamos escolher qualquer valor para $x_0$, e algumas escolhas poderiam fazer a convergência para $x$ ser ainda mais rápida, porém, a condição inicial para $r_0$ e $d_0$ é $d_0 = r_0 = b - Ax_0$, o que faz bastante sentido, já que inicialmente ambos se referem à distância de $x_0$ para $x$. Porém, se escolhermos $x_0 = 0$, trivialmente, $b - Ax_0 = b$, já que $A$ é positiva definida. \\
A convergência em $n$ passos é teórica. Os erros de precisão de ponto flutuante fazem com que os espaços $\mathcal{K}_i$ não sejam necessáriamente ortogonais. Com o acúmulo de operações isso faz com que o método de Gradientes Convergences sofra, mesmo que numa escala bem menor, do mesmo mal que o método \textit{Steepest Descent}.

\section{Implementação}
\subsection{Matrizes Esparsas}
Grande parte do objetivo de usar usar métodos iterativos para resolver sistemas lineares é poder tomar vantagem de propriedades específicas delas, como, por exemplo, a esparsidade. \\
Neste caso, podemos diminuir a complexidade do que, num caso normal, seria o gargalo do nosso algoritmo. A multiplicação $Ad_{i-1}$ a toda iteração do método GC. Para isso, basta lembrar da definição de multiplicação de uma matriz por um vetor:
$$ Ad = g \implies g_i = \sum\limits_{j=1}^n A_{i,j}d_j $$
Ou seja, toda linha de $g$ é uma combinação linear de $d$ onde os coeficientes são dados por elementos de $A$. Assim, se conseguirmos evitar processar os elementos nulos de $A$ e percorrermos só os elementos não nulos teremos uma melhoria razoável na performance. Para isso, basta representar a matriz como $n$ listas ligadas, representando as linhas, que tem como valor em cada nó a coluna e o valor de cada item não nulo. \\
Além da melhoria no tempo, essa representação otimiza também o espaço, já que não precisa guardar todos os elementos não nulos da matriz. Assim, o custo de cada multiplicação da uma matriz esparsa por um vetor bem como o espaço usado pela matriz é precisamente $n+m$ onde $n$ é a quantidade de linhas da matriz e $m$ é a quantidade de elementos não-nulos de $A$. \\

\subsection{Convergência}
Já que, como foi dito nas observações, na prática é impossível garantir convergência em $n$ passos, foi definido um $\epsilon$ de $10^{-9}$ e, na implementação, é considerado que o método convergiu quando $r^Tr \leq \epsilon$. Ou seja, quando a norma 2 de $r$ for suficientemente pequena. \\

\section{Testes e Comparação com Cholesky}
Agora é hora de testar o método com alguns casos. Também é muito importante comparar a realização desse método com o outro que conhecemos para matrizes positivas definidas, a decomposição de Cholesky. \\
É imporatnte notar que as comparações foram feitas com a implementação de Cholesky orientada a linhas, a mais eficiente para C e C++.

\subsection{Comparação à Priori}
É importante notar que o método iterativo não diz se a matriz de entrada é positiva definida. Diferente da decomposição de Cholesky que consegue garantir que a matriz de entrada é positiva definida e interromper a execução caso ela não seja, o método GC não verifica isso. \\
Além disso, se tivermos vários sistemas do tipo $Ax = b$ com uma $A$ única porém vários vetores $b$ diferentes a decomposição de Cholesky só precisa ser realizada uma vez e todas os $b$ são descobertos, a partir dai, com $O(n^2)$ operações. Os métodos iterativos não podem reaproveitar a resposta anterior desta maneira, gerando mais uma disvantagem em realção aos métodos diretos. \\

\subsection{Comparação à Posteriori}
Dado o $x$ calculado pelo GC eu tentei comparar o resultado de $Ax$ com $b$ mas a comparação sempre retornava $0$, mesmo com respostas que ficavam diferentes das dadas pela decomposição de Cholesky, então eu provavelmente fiz mais alguma coisa errada e, infelizmente, não consegui arrumar. Assim, ficou faltando uma análise do erro cometido pela minha implementação da GC, o que seria interessante.

\subsection{Gerando os Testes}
Para criar os testes eu usei o \textit{genmatsim.c} do ep1 e criei alguns programas novos que geram matrizes esparsas segundo o exemplo do livro e de uma outra maneira diferente (que será explicada). \\
Para rodar todos esses programas foi criado um pequeno script em bash, para usá-lo entre na pasta \textit{codigo/testes} e execute o arquivo \textit{generateTestes.sh}. É importante que você esteja dentro da pasta onde está este arquivo para que ele funcione. Também é interessante notar que ele vai gerar matrizes grandes que vão ocupar algum espaço, porém nenhum dos arquivos gerados é absurdamente grande nem demora muito tempo.

\subsection{Matrizes Pequenas}
Primeiro foram usadas algumas matrizes pequenas para testar meu programa. Elas estão disponíveis em \textit{codigo/testes/pequenas} e não é necessáiro usar o gerador para isso. \\
\begin{tabular}{| l | l | l | l | l | l | l |}
    \hline
    Entrada & $n$a & $m$ & Iterações GC & Tempo GC & Tempo Cholesky \\ \hline
    \textit{2.in} & $2$ & $4$ & $2$ & $0.007$s & $0.006$s \\ \hline
    \textit{3.in} & $3$ & $9$ & $3$ & $0.008$s & $0.005$s \\ \hline
    \textit{id.in} & $10$ & $10$ & $1$ & $0.005$s & $0.006$s \\ \hline
\end{tabular} \\
Mesmo com matrizes pequenas é possível ver a diferença entra GC e Cholesky. As duas primeiras matrizes são completas de tamanho muito pequeno, para elas, a GC converge em $n$ passos e é consistentemente menos eficiente do que Cholesky com diferenças na ordem de $0.001$s. Isto ocorre provavelmente por causa da estrutura de lista ligada e claramente não é relacionado à complexidade do algoritmo. \\
O terceiro exemplo é uma matriz identidade de tamanho $10$. Nessa matriz, o GC acha a resposta em $1$ passo e, por isso, é mais rápida que Cholesky. De fato, a GC encontra a resposta em uma iteração sempre que $b$ é um autovetor de $A$ ou quando os autovetores de $A$ são todos iguais ~\cite[p.~33]{shewchuk1994introduction}. \\
Esses testes servem apenas para mostrar casos genéricos e o caso específico explicado acima exemplificado pela matriz identidade.

\subsection{Exemplo do Livro}
O livro \cite{trefethen1997numerical} mostra uma maneira de gerar matrizes esparsas de forma aleatória. Este método foi implementado em \textit{codigo/testes/exemplo.cpp} e 3 casos foram testados aqui. Eles são interessantes pois é possível compará-los com Cholesky, já que as matrizes não são tão grandes. \\
\begin{tabular}{| l | l | l | l | l | l | l |}
    \hline
    Entrada & $\lambda$ & $n$ & $m$ & Iterações GC & Tempo GC & Tempo Cholesky \\ \hline
    \textit{01.in} & $0.01$ & $500$ & $2585$ & $6$ & $0.01$s & $0.38$s \\ \hline
    \textit{02.in} & $0.02$ & $500$ & $5089$ & $9$ & $0.02$s & $0.39$s \\ \hline
    \textit{1.in} & $0.1$ & $500$ & $25177$ & $\infty$ & $\infty$ & $0.42$s \\ \hline
\end{tabular} \\
Os testes acima receberam um parâmetro $\lambda$ como sugerido no livro. É possível com um $\lambda$ pequeno, o tempo da GC é muito melhor do que o da decomposição de Cholesky. Apesar disso, com $\lambda = 0.1$, o método divergiu e isso, provavelmente, ocorre por causa da propagação do erro nas contas.

\subsection{Matrizes Densas}
Estas matrizes são geradas pelo \textit{genmatsim.c}, o mesmo que gerou os exemplos de teste para o EP1. É interessante comparar elas com a decomposição de Cholesky pois foi com elas que testamos os métodos diretos pela primeira vez. \\
\begin{tabular}{| l | l | l | l | l | l | l |}
    \hline
    Entrada & $n$ & $m$ & Iterações GC & Tempo GC & Tempo Cholesky \\ \hline
    \textit{100.in} & $100$ & $n^2$ & $155$ & $0.08$s & $0.04$s \\ \hline
    \textit{500.in} & $500$ & $n^2$ & $971$ & $9.92$s & $1.03$s \\ \hline
    \textit{700.in} & $1309$ & $n^2$ & $971$ & $25.36$s & $2.31$s \\ \hline
\end{tabular} \\
É fácil ver que o comportamento da GC para matrizes densas é catastrófico. Já que o método de GC usa estruturas de dados mais complexas e tem muito mais processamento por iteração, ela é uma péssima escolha para os casos em que a decomposição de Cholesky é utilizável, perdendo em tempo, espaço e precisão.

\subsection{Matrizes Esparsas}
Como pedido no enunciado do EP, eu tenho que criar um gerador de matrizes positivas definidas esparsas. Com alguma busca eu encontrei a discussão \cite{darylmathexchange} que dá um método bem parecido com o fornecido pelo livro \cite{trefethen1997numerical} para gerar esse tipo de matriz. \\
Eu tentei, então, replicar o método apresentado em \cite{darylmathexchange} de forma mais rápida e que não gastasse memória. Para isso, eu gerei $n$ reais da forma $n+\alpha$ onde é aleatório e uniforme entre $0$ e $1$ que são os elementos inseridos nas diagonais. \\
Depois disso, eu gerei trincas de elementos $x, y, \alpha$ tais que $x$ e $y$ são inteiros uniformemente distribuídos entre $0$ e $n-1$ e $\alpha$ é um real uniformemente distribuído entre $-1$ e $1$. Com as trincas geradas, eu inseri na matriz nas posições $x,y$ e $y,x$ o valor $\alpha$. \\
Existem alguns problemas com este método. Eu não sei garantir (e provavelmente não é possível) que a matriz gerada seja positiva definida, apenas existe a garantia de que ela é simétrica, esparsa e muito parecida com um multiplo da matriz identidade. É razoável de que isso funcione bem para matrizes esparsas e terrívelmente mal para matrizes densas. \\
Além disso, é possível gerar duas vezes a mesma dupla $x,y$, ou até mesmo gerar duas duplas da forma $x,y$ e $y,x$ em momentos diferentes, obtendo o mesmo efeito. Com isso, a minha implementação de GC vai se comportar como se os valores gerados em cada uma das vezes fossem somados à posição, o que difícilmente é um problema. \\
\begin{tabular}{| l | l | l | l | l | l | l |}
    \hline
    Entrada & $n$ & $m$ & Iterações GC & Tempo GC \\ \hline
    \textit{1e1\_7.in} & $10$ & $24$ & $7$ & $0.006$s \\ \hline
    \textit{1e2\_10.in} & $100$ & $120$ & $4$ & $0.016$s \\ \hline
    \textit{1e5\_200.in} & $10^5$ & $10^5+400$ & $2$ & $0.873$s \\ \hline
    \textit{1e6\_307.in} & $10^6$ & $10^6+614$ & $2$ & $6.283$s \\ \hline
\end{tabular} \\
Não vale a pena comparar a performance dos resultados acima com Cholesky, pois para as matrizes pequenas já fizemos comparações suficientes e para as duas ultimas, as matrizes grandes, é inviável realizar a decomposição. \\
Esses exemplos mostram a potência dos métodos iterativos para a resolução de sistemas desse tipo. As matrizes de entrada têm densidade baixíssima e o método dos Gradientes Conjugados conseguiu tirar muito proveito disso, resolvendo os dois ultimos sistemas em duas iterações cada. É fácil ver nesses exemplos que a quantidade de iterações foi inversamente proporcional à densidade da matriz, quanto maior a proporção $m/n$, maior a quantidade de iterações feitas. \\
Não foi possível executar um exemplo com uma matriz de $n$ da ordem de $10^7$ pois isso gasta uma quantidade muito grande de memória e as operações de alocação dinâmica e leitura da entrada ficam bastante custosas, mas o desempenho para $10^6$ mostra uma realidade feliz quanto ao funcionamento do método. \\
Tamanhos matrizes de dimensão $10^4$ são quase intratáveis, tanto pela memória quanto pelo tempo, para algoritmos diretos, mas, com uma matriz devidamente esparsa, o método dos GC conseguiu resolver com facilidade o sistema para uma matriz de dimensão $10^7$, mostrando uma grande vantagem em relação aos métodos anteriormente conhecidos.


%----------------------------------------------------------------------------------------
%   BIBLIOGRAFIA
%----------------------------------------------------------------------------------------

\renewcommand{\refname}{\spacedlowsmallcaps{Bibliografia}} % For modifying the bibliography heading
\bibliography{relatorio.bib} % The file containing the bibliography
\bibliographystyle{unsrt}

%----------------------------------------------------------------------------------------

\end{document}
