%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Arsclassica Article
% LaTeX Template
% Version 1.1 (10/6/14)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% Lorenzo Pantieri (http://www.lorenzopantieri.net) with extensive modifications by:
% Vel (vel@latextemplates.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[
10pt, % Main document font size
a4paper, % Paper type, use 'letterpaper' for US Letter paper
oneside, % One page layout (no page indentation)
%twoside, % Two page layout (page indentation for binding and different headers)
headinclude,footinclude, % Extra spacing for the header and footer
BCOR5mm, % Binding correction
]{scrartcl}

\input{structure.tex} % Include the structure.tex file which specified the document structure and layout

\hyphenation{Fortran hy-phen-ation} % Specify custom hyphenation points in words with dashes where you would like hyphenation to occur, or alternatively, don't put any dashes in a word to stop hyphenation altogether

%----------------------------------------------------------------------------------------
%	TITLE AND AUTHOR(S)
%----------------------------------------------------------------------------------------

\title{\normalfont\spacedallcaps{Método dos Gradientes Conjugados}} % The article title
\subtitle{\normalfont\spacedallcaps{EP2 - MAC0300}}

\author{\spacedlowsmallcaps{Victor Sena Molero (8941317)}} % The article author(s) - author affiliations need to be specified in the AUTHOR AFFILIATIONS block

%----------------------------------------------------------------------------------------

\begin{document}

%----------------------------------------------------------------------------------------
%	HEADERS
%----------------------------------------------------------------------------------------

\renewcommand{\sectionmark}[1]{\markright{\spacedlowsmallcaps{#1}}} % The header for all pages (oneside) or for even pages (twoside)
%\renewcommand{\subsectionmark}[1]{\markright{\thesubsection~#1}} % Uncomment when using the twoside option - this modifies the header on odd pages
\lehead{\mbox{\llap{\small\thepage\kern1em\color{halfgray} \vline}\color{halfgray}\hspace{0.5em}\rightmark\hfil}} % The header style

\pagestyle{scrheadings} % Enable the headers specified in this block

%----------------------------------------------------------------------------------------
%	TABLE OF CONTENTS & LISTS OF FIGURES AND TABLES
%----------------------------------------------------------------------------------------

\maketitle % Print the title/author/date block

\setcounter{tocdepth}{2} % Set the depth of the table of contents to show sections and subsections only

\tableofcontents % Print the table of contents

%\listoffigures % Print the list of figures
%\listoftables % Print the list of tables

%----------------------------------------------------------------------------------------
%	ABSTRACT
%----------------------------------------------------------------------------------------

\section*{Abstract} % This section will not appear in the table of contents due to the star (\section*)

Este é um relatório sobre a implementação do método dos Gradientes Conjugados para encontrar um $x \in \mathbb{R}^n$ tal que $Ax = b$ onde $A \in \mathbb{R}^{n \times n}$ é esparsa, positiva definida e conhecida e $b \in \mathbb{R}^n$ é conhecida.

%----------------------------------------------------------------------------------------

\newpage % Start the article content on the second page, remove this if you have a longer abstract that goes onto the second page

%----------------------------------------------------------------------------------------
%	INTRODUCTION
%----------------------------------------------------------------------------------------

\section{Introdução}
Durante o semestre foram estudadas várias formas de se resolver sistemas da forma $Ax = b$, é fácil perceber que, com hipóteses mais fortes, esse tipo de cálculo se torna mais fácil. Por exemplo, se $A$ for positiva definida, podemos aplicar a decomposição de Cholesky e evitar o cálculo da decomposição LU, que funciona, também, no caso onde $A$ é positiva definida. \\
O método dos Gradientes Conjugados tem, também, como hipótese, uma $A$ positiva definida. Porém, esse método permite que propriedades da matriz $A$, como, por exemplo, a esparsidade, sejam aproveitados. \\
O objetivo desse relatório é, então, explicitar o que foi entendido sobre o método em questão, mostrar uma forma de aproveitar a esparsidade da matriz no método dos Gradientes Conjugados e discutir a complexidade desse método, comparando o com a decomposição de Cholesky, já que esta é derivada das mesmas hipóteses que o método dos Gradientes conjugados. Além disso, pretendo explicar o que foi feito em outras partes do desenvolvimento, como, por exemplo, o método para gerar matrizes positivas definidas esparsas. \\
Para tentar entender o método, foram estudados dois textos, o livro \textit{Numerical Linear Algebra}\cite{trefethen1997numerical}, como indicado no enunciado do EP e um paper intitulado \textit{Painless Conjugate Gradient}\cite{shewchuk1994introduction}.
 
%----------------------------------------------------------------------------------------
%   GRADIENTES CONJUGADOS
%----------------------------------------------------------------------------------------

\section{Gradientes Conjugados}
O método dos Gradientes Conjugados, em \cite{shewchuk1994introduction} é apresentado como uma derivação dos métodos \textit{Steepest Descent} e \textit{Conjugate Directions}. Vou introduzir ambos brevemente aqui, para mostrar como eu entendi o método dos Gradientes Conjugados.

\subsection{Steepest Descent}
O método do \textit{Steepest Descent} busca encontrar o $x$ que tem a forma quadrática mínima com a matriz $A$ e o vetor $b$ dados no problema. \\
A forma quadrática de um vetor é uma função escalar e quadrática de um vetor da forma ~\cite[p.~2]{shewchuk1994introduction}
$$ f(x) = \frac{1}{2}x^{T}Ax - b^{T}x + c $$ 
E o gradiente da forma quadrática é simplesmente ~\cite[p.~3]{shewchuk1994introduction}
$$ f'(x) = Ax - b $$ 
Assim, é fácil ver que se $ f'(x) = 0 $ então
$$ Ax = b $$
resolvendo o problema. \\
Para uma $A$ positiva definida, é possível mostrar que só existe um mínimo para a forma quadrática ~\cite[p.~5]{shewchuk1994introduction} e não existe máximo, assim, calcular $f'(x) = 0$ é simplesmente minimizar a função $f(x)$ no seu único ponto de mínimo. Assim, seguir o gradiente maximiza a função, se seguirmos a direção oposta ao gradiente, estaremos nos aproximando cada vez mais do mínimo da função. E é nisso que o método se baseia. \\
Assim, o método chuta um ponto $x_0$ e, para calcular $x_{i+1}$, para todo $i$, vai na direção oposta à função $f'(x_i)$ o que garante que, com o tamanho certo do passo nessa direção, $\lim_{i \to \infty} f'(x_i) = 0$. \\
O problema é que este método pode dar passos na mesma direção que métodos anteriores, pois a direção dos passos é definida pelo gradiente de $f$. De fato, isso acontece na prática. O método de \textit{Conjugate Directions} impede que isso aconteça.

\subsection{Conjugate Directions}
Como dito acima, este método garante que nenhum as direções de cada passo sejam diferentes entre si. Para fazer isso, é introduzida a ideia de direção de busca. O método usa direções de busca $d_i$ diferentes entre si e sempre minimiza $f'(x_{i+1})$ com alterações do tipo $x_{i+1} = x_i + \alpha_i d_i$ para cada passo $i$. \\
Para isso, as direções geradas são \textit{A-ortogonais} entre si, ou seja, são tais que 
$$ \forall i,j \text{ t.q. } i \neq j\text{, } d_i^{T}Ad_j = 0 $$
Isso faz com que nenhum passo seja dado na mesma direção, já que elas são todas diferentes, e também faz com que as direções $d_0, d_1, \dots, d_i$ gerem um espaço vetorial de dimensão $i$, que chamaremos $\mathcal{D}_i$, como em \cite{shewchuk1994introduction}. Logo, se $x$ tem dimensão $n$, $x \in \mathcal{D}_n$. E já que $f'(x)$ é minimizado a cada iteração, $x_i$ é tal que $\forall p \in \mathcal{D}_i f(x_i) \leq f(p)$, ou seja, $x_i$ é o ponto mínimo de $f$ para o subespaço $\mathcal{D}_i$. \\
Agora falta conseguirmos $n$ direções de busca ${d_i}$ que são \textit{A-ortogonais} (ou seja, conjugadas) entre si. Para isso, o método de Conjugate Directions usa o processo de \textit{Gram-Schmidt} \cite[p.~25]{shewchuk1994introduction}. O problema é que, para aplicar esse método, todos os vetores já gerados devem ser guardados e percorridos completamente para gerar o próximo vetor, assim, naturalmente, o processo custa, em quantidade de operações $O(n^3)$ e $O(n^2)$ em espaço. \\

\subsection{Espaços de Krylov e o Método dos Gradientes Conjugados}
Felizmente, existe uma maneira mais eficiente de gerar as direções de busca ${d_i}$. É isso que o método dos gradientes conjugados apresenta com os espaços de Krylov. Denotaremos por $\mathcal{K}_i$ um subespaço de Krylov de dimensão $i$. \\
A facilidade para se calcular $\mathcal{K}_{i+1}$ dado $\mathcal{K}_{i}$ fez com que estes subespaços sejam a base pra vários algoritmos iterativos para resolução de sistemas lineares e para o cálculo de autovalores, como mostrado em \cite{trefethen1997numerical}. Dados uma matriz $A$ e um vetor $b$, o espaço de $\mathcal{K}_i$ tem como base $[b, Ab, A^2b, \dots, A^{i-1}b]$ ~\cite[p.~245]{trefethen1997numerical}. \\
Assim, o método dos Gradientes Conjugados é exatamente o \textit{Conjugate Directions} que usa o subespaço de Krylov para gerar direções de busca ortogonais entre si. \\

\subsection{O Algoritmo}
Agora temos tudo o que precisamos para derivar o algoritmo. Primeiro, vamos definir quais são as informações que precisamos ter em cada passo e o que elas significam. \\
\begin{description}
    \item[$A$] \hfill \\
    Entrada do Problema. $A \in \mathbb{R}^{n \times n}$ é simétrica, positiva definida e tem $m$ elementos não-nulos (esparsa).
    \item[$b$] \hfill \\
    Entrada do Problema. $A \in \mathbb{R}^{n}$.
    \item[$x_i$] \hfill \\
    Aproximação atual do valor de $x$ tal que $Ax = b$.
    \item[$r_i$] \hfill \\
    Residual de $b$, é usado para gerar a direção de Busca.
    \item[$d_i$] \hfill \\
    Direção de Busca. Onde serão gerados, indiretamente dos espaços $\mathcal{K}_i$.
    \item[$\beta_i$] \hfill \\
    Razão entre a norma do residual no passo $i$ e $i-1$, mostra o quão mais próximo da resposta $x$ $x_i$ está em relação a $x_{i-1}$.
\end{description}
O algoritmo, então, é o seguinte: ~\cite[p.~294]{trefethen1997numerical}
\begin{algorithm}[]
    $x_0 = 0, r_0 = b, d_0 = r_0$ \\
    \For{$i = 1,2,\dots,n$}{
        $\alpha_i = (r_{i-1}^Tr_{i-1})/(d_{i-1}^T A d_{i-1})$ \\
        $x_i = x_{i-1} + \alpha_i d_{i-1} $ \\
        $r_i = r_{i-1} - \alpha_i A d_{i-1} $ \\
        $\beta_i = (r_{i-1}^Tr_{i-1})/(r_{i}^Tr_{i})$ \\
        $d_i = r_i + \beta_i d_{i-1}$
    }
\end{algorithm}

\subsection{Observações}
Algumas observações interesantes podem ser feitas sobre o algoritmo. \\
Os espaços $\mathcal{K}_i$ não são explícitamente calculados. Mas eles são deles que partem as direções de busca. É fácil ver que, de fato, a direção de busca é uma combinação linear de $b, Ab, A(Ab), \dots, A^{i-1}$ em todo passo $i$. Inicialmente $d_0$ = $b$, ou seja, $d_0 \in \mathcal{K}_0$, como esperado. No passo $i$, calculamos $r_i$ e, com ele, calculamos $d_i$. É fácil ver que, no primeiro passo:
$$ d_1 = b - \alpha_1 Ab + \beta_1 b = (1+\beta_1)b - \alpha_1 Ab $$
Logo, $d_1$ é uma combinação linear de $Ab$ e $b$, pertencendo a $\mathcal{K}_1$. Isso continua por toda a execução do algoritmo. \\
Poderíamos escolher qualquer valor para $x_0$, e algumas escolhas poderiam fazer a convergência para $x$ ser ainda mais rápida, porém, a condição inicial para $r_0$ e $d_0$ é $d_0 = r_0 = b - Ax_0$, o que faz bastante sentido, já que inicialmente ambos se referem à distância de $x_0$ para $x$. Porém, se escolhermos $x_0 = 0$, trivialmente, $b - Ax_0 = b$, já que $A$ é positiva definida. \\

\section{Matrizes Esparsas}

% Caso A = I

%----------------------------------------------------------------------------------------
%   BIBLIOGRAFIA
%----------------------------------------------------------------------------------------

\renewcommand{\refname}{\spacedlowsmallcaps{Bibliografia}} % For modifying the bibliography heading
\bibliography{relatorio.bib} % The file containing the bibliography
\bibliographystyle{unsrt}

%----------------------------------------------------------------------------------------

\end{document}
